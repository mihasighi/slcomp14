We thank the reviewers for their careful reading and 
the thoughtful suggestions. We provide below a commented list of 
changes done to the paper in order to improve it.


> Reviewer 1
>++++++++++
> ...
> Overall, I see two points for improvements:

> The introduction of syntax and semantics is too extensive and
> not really used in the rest of the paper. The authors try to introduce
> a complex logic in a compact way and that makes it hard to read
> and I doubt that in this paper it adds any value to the reader.
> At least it should be better related to the given examples, but
> I have the impression that the informal introduction is sufficient.

We have changed the presentation of the logic. We kept only
the informal introduction and we added some examples and explanations 
of the inductive definitions.
 
> The results of the competition are not very carefully analysed.
> Only some accumulated values are given. It would be interesting
> to see the differences between the different benchmark sets and
> get a better comparison of the solver’s behavior. For example, cactus
> plots as in common in the SAT competition could be used.

Thanks for these suggestions. We have updated Section 4 (Results) with
more comments on the insights provided by the competition on
the benchmarks and solvers.

[TODO: something about cactus plots, David?]

> Details:

> The abstract emphasizes that 3 solvers are using SAT/SMT. Why is
> this so important to mention that in the abstract?

We wanted to emphasize the relation between this competition and 
the SAT & SMT competitions. Notice that some solvers are also based 
on techniques that are not classic for SAT/SMT, e.g., tree automata.
Finally, we removed this sentence from the abstract.

> It is good that concrete examples are give (Table 1). But they should
> be better integrated in the text.

We have provided explanations for each example in the presentation of the logic.

> For the syntax, it would be helpful to have the alphabets introduced.

Only the informal presentation of the logic has been kept.
Some alphabets were introduced informally.

> There is a sentence „We solicited the support of D. Cock“ As he is
> one of the co-authors of this paper this reads a little odd, please
> rephrase this.

Done.


> Reviewer 2
> ++++++++++

> The paper describes the competition of separation logic solvers arranged 
> as an unofficial satellite event of the FLOC 2014 Olympic Games. 
> The paper is a carefully written description of the competition, 
> which seems to have been organized for the first time. 
> Because of this, much of the content of the paper are on basic things such as 
> the logic fragments being supported and the practicalities of running the benchmarking, 
> while the analysis of the competition outcome is left a bit lighter. 

The presentation of results has been improved as suggested by the reviewers
(see points below).

> Thankfully the complexity issues and syntax + semantics are covered.
 
> The part about the tool internal techniques is a bit short and could be extended. 
> Especially analysis on which techniques worked for which benchmark categories 
> could be insightful. 

We added an analysis of the results obtained with each technique in Section 4 (Results).

> Still I think the paper is of interest to the separation logic community, 
> and thus should be accepted.

> All in all I think the paper is almost ready to be published. Below I list 
> some minor things the authors could improve on for the next version of the submission.

> - On page 1: Please use "has motivated the development" instead of "motivated the development", 
> as the development of tools is still ongoing and is not a thing of the past.
> - On page 2: "using recursive definition" should be "using recursive definitions", 
> as there are several recursive definitions in place.
> - Page 3 is a bit empty with lost of free space above and below Table 1, 
> see if you could fit some more content on that page somehow.     
> - The bullet point list in the middle of page 7 should have an ", and" 
> at the end of the second to last bullet.
> - Page 9 and elsewhere in the paper: There are very short (less than a line) long paragraphs, 
> please make the paragraphs a couple of lines long at least.

All these suggestions have been implemented in the current version.

> - On page 5 there are several list, some of which use a ":" before the first "(i)" item, 
> while others do not. Please make the punctuations of lists consistent within the paper. 
> Also on other pages, like page 7, go through the whole paper.

We changed some punctuations in the paper in order to obtain uniformity.

> - Page 9 discusses the competition scoring, including counting the number of incorrect results. 
> However, I could not find any discussion of incorrect results elsewhere in the paper. 
> Were any discrepancies found? What was the procedure to handle those, could solver submitters 
> resubmit fixed solvers? Should this data be in Table 4? 
> I guess so, as otherwise I can not interpret the results. 
> This is my main complaint, all of the (a)-(c) data is not provided to the readers in Table 4, 
> just (b) and (c)!
> - Were any "training runs" run to weed out buggy solvers? How was the final set of benchmarks selected?

We added in Section 3.4 more information about the dealing of discrepancies on obtained results.
Also, we indicated the number of training runs (1). Because these runs were 
unofficial, we did not have any data about the number of bugs fixed in solvers.

The selection of the final set of benchmarks is commented in a paragraph of Section 3.3.
We have added more explanations concerning the computation of the status (expected result) 
of each problem. 

The table presenting the overall results includes now the point (a) and recalls
the total number of problems for each division.

> - Page 10: The sentence: "We make a special mention..." could be reworded, it sounds a bit clumsy to my ear.

We removed this sentence and added comments on each participating solver.

> - Please make the SMT-LIB spelling consistent, both "SMTLIB" and "SMT-LIB" are used.
> - Page 12: "with smallest size" -> "with the smallest number of benchmarks"

Done.

> - Question: Is there any automated way to handle discrepancies, how were the buggy solvers 
> found out when there were inconsistencies between them? Is any further work needed on this side?

No, we don't have an automatic way to handle discrepancies.
When inconsistent results were signaled by one solver, we check it with the other 
solvers. Notice that some solvers (CYCLIST, SLSAT) produce a proof tree,
which can also be used to check the result when the problem could be checked by 
these solvers. In the future works we mentioned that a diagnosis output would be 
interesting for this task.


Reviewer 3
++++++++++

> Organizing a competition, especially the first one for a community, is
> a difficult task. Even if the scientific results do not seem to be
> that interesting, one of the main interest of this article is to set
> the basis for an independent evaluation of SL solvers.

> My first comment is about the description of the SLRD fragment. For
> someone who does not know the field, the description of the input
> theory is quite obscure. It is of course useful to give an exact
> description of the theory fragment that was used in the competition.
> Nevertheless, a rather informal description of what can be represented
> with the theory, and what it allows to prove, would be welcome. I
> guess that SLRD could be introduced to newbies in about half a page,
> referring the reader to introduction papers for further details.

> In the syntax description, the definition of "set of field references" is
> extremely confusing: what does \ro \cups \ro actually mean?

We have changed the presentation of the logic as suggested.
We provide the alphabets, introduce the formulas intuitively and
provide some examples.

> It would be worth explaining at least one of the recursive definitions
> of Table 1.

We have explained all the recursive definitions in Table 1.

> In table 3, it is quite surprising that no industrial benchmark
> appears. A comment on this would be welcome.

At the time, we were not aware about any industrial tool using SL solver.
In the future, we hope to obtain such benchmarks from new industrial tools 
like the Monoidics tool (now owned by FaceBook).
These comments have been introduced in Section 3.3 and in the 'Future work' 
paragraph of the Conclusion.

> Table 4 requires more explanations. Is the CPU time given in seconds?
> I understand that the time reported is the cumulated time over all
> solved instance. It is also important to recall in this table the total number of
> instances.

We improved the presentation in Table 3 (Overall results).

> Several solvers seem to be very fast (for example
> Asterix, in the SLL => division, solves 292 instances in 2.98 seconds,
> which is approximately 10 ms per instance). This is quite
> surprising. Does this mean that most instances are small and very
> easy?  In the two SLL divisions, 3 solvers were able to solve
> every instance. Are the problems too easy? 

We discussed in Section 2 that for the SLL divisions, the logic fragment has
decidable satisfiability and entailment problems and very efficient (NPTIME)
decision procedures have been proposed.
This explains in part the very good results obtained.
We added a comment on "Future work" paragraph about including more difficult 
problems (in size of the problem) in this division.

> In the UDB => division,
> sleek solves more instances than cyclist-sl, but does not appear in
> bold face. Does it mean that it was incorrect on a few instances? If
> some solvers were unsound in some categories, this should be reported.

We added the incorrect answers in Table 3 (Overall results).

> Can we tell from Table 4 if some techniques are more successful or
> promising than others?

We added an analysis of the results obtained with each technique in
Section 4 (Results).

> How do you plan to assign a measure of difficulty to each instance?

For the divisions where the problems are decidable (SLL* and FDB), the
difficulty could be measured in the size of the formulas and the
inductive definitions used.
In addition to the size information, the difficulty for the UDB problems
could be computed from the features used in the inductive definitions, e.g.,
the number of fields in the data structures their ``regularity''.
For example, the btree definition is less difficult than the tll definition
(see Table 1).

> Most instances seem to be quite easy (from Table 4). Don't you need
> more challenging/industrial instances?

In addition to the above comment on the SLL divisions, please notice that
the problems in the FDB and UDB divisions could not be solved by all solvers
(they timeout or exhausted the memory). The solved instances show the efficiency 
of some decision procedure/heuristics on these instances.

> The sentence "The benchmark problems used in the competition were
> split into five divisions that correspond to the three classes of
> problems above and the two decision problems considered." is so
> surprising (5!=3*2) that I immediately stopped and missed the
> explanation in the next sentence. I suggest rewriting it (for example
> : "out of the 6 possible divisions (three classes of problems above
> and the two decision problems considered), only five were opened
> because...").

We rephrased the sentence like suggested.

> The limits (2400 s and 100GB) imposed to a solver are unclear to
> me. Is it 2400 s CPU time or wall-clock time (BTW, is there any
> parallel solver?)? Is it 100GB RAM (huge!) or disk space?

Both CPU and wall-clock times were set to the limit and no parallel solver
participated to this edition. Some of jobs (i.e., solver on problem) exceed 
these limits and its result considered then as unknown.
The memory limit is indeed in RAM.
We added these particular details in Section 3.4.

> I think it's worth indicating which parts of the format need some
> readjustments (what are the expected evolutions ?).

We included some explanations for this item.

> The description of the problem raised by SeLoger (batch mode) is unclear.

We rephrased the description.

