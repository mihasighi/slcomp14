Reviewer 1
++++++++++

The paper gives a nice overview on the efforts taken
to organize the SL competition. The paper is clearly structured
and well written and provides a good documentation of the SL
competition. The paper fits very well in the scope of the special
issue.

Overall, I see two points for improvements:

The introduction of syntax and semantics is too extensive and
not really used in the rest of the paper. The authors try to introduce
a complex logic in a compact way and that makes it hard to read
and I doubt that in this paper it adds any value to the reader.
At least it should be better related to the given examples, but
I have the impression that the informal introduction is sufficient.

The results of the competition are not very carefully analysed.
Only some accumulated values are given. It would be interesting
to see the differences between the different benchmark sets and
get a better comparison of the solver’s behavior. For example, cactus
plots as in common in the SAT competition could be used.

Details:

The abstract emphasizes that 3 solvers are using SAT/SMT. Why is
this so important to mention that in the abstract?

It is good that concrete examples are give (Table 1). But they should
be better integrated in the text.

For the syntax, it would be helpful to have the alphabets introduced.

There is a sentence „We solicited the support of D. Cock“ As he is
one of the co-authors of this paper this reads a little odd, please
rephrase this.


Reviewer 2
++++++++++

The paper describes the competition of separation logic solvers arranged as an unofficial satellite event of the FLOC 2014 Olympic Games. The paper is a carefully written description of the competition, which seems to have been organized for the first time. Because of this, much of the content of the paper are on basic things such as the logic fragments being supported and the practicalities of running the benchmarking, while the analysis of the competition outcome is left a bit lighter. Thankfully the complexity issues and syntax + semantics are covered. The part about the tool internal techniques is a bit short and could be extended. Especially analysis on which techniques worked for which benchmark categories could be insightful. Still I think the paper is of interest to the separation logic community, and thus should be accepted.

All in all I think the paper is almost ready to be published. Below I list some minor things the authors could improve on for the next version of the submission.

- On page 1: Please use "has motivated the development" instead of "motivated the development", as the development of tools is still ongoing and is not a thing of the past.

- On page 2: "using recursive definition" should be "using recursive definitions", as there are several recursive definitions in place.

- Page 3 is a bit empty with lost of free space above and below Table 1, see if you could fit some more content on that page somehow.

- On page 5 there are several list, some of which use a ":" before the first "(i)" item, while others do not. Please make the punctuations of lists consistent within the paper. Also on other pages, like page 7, go through the whole paper.

- The bullet point list in the middle of page 7 should have an ", and" at the end of the second to last bullet.

- Page 9 and elsewhere in the paper: There are very short (less than a line) long paragraphs, please make the paragraphs a couple of lines long at least.

- Page 9 discusses the competition scoring, including counting the number of incorrect results. However, I could not find any discussion of incorrect results elsewhere in the paper. Were any discrepancies found? What was the procedure to handle those, could solver submitters resubmit fixed solvers? Should this data be in Table 4? I guess so, as otherwise I can not interpret the results. This is my main complaint, all of the (a)-(c) data is not provided to the readers in Table 4, just (b) and (c)!

- Were any "training runs" run to weed out buggy solvers? How was the final set of benchmarks selected?

- Page 10: The sentence: "We make a special mention..." could be reworded, it sounds a bit clumsy to my ear.

- Please make the SMT-LIB spelling consistent, both "SMTLIB" and "SMT-LIB" are used.

- Page 12: "with smallest size" -> "with the smallest number of benchmarks"

- Question: Is there any automated way to handle discrepancies, how were the buggy solvers found out when there were inconsistencies between them? Is any further work needed on this side?


Reviewer 3
++++++++++

Organizing a competition, especially the first one for a community, is
a difficult task. Even if the scientific results do not seem to be
that interesting, one of the main interest of this article is to set
the basis for an independent evaluation of SL solvers.

My first comment is about the description of the SLRD fragment. For
someone who does not know the field, the description of the input
theory is quite obscure. It is of course useful to give an exact
description of the theory fragment that was used in the competition.
Nevertheless, a rather informal description of what can be represented
with the theory, and what it allows to prove, would be welcome. I
guess that SLRD could be introduced to newbies in about half a page,
referring the reader to introduction papers for further details.

In the syntax description, the definition of "set of field references" is
extremely confusing: what does \ro \cups \ro actually mean?

It would be worth explaining at least one of the recursive definitions
of Table 1.

In table 3, it is quite surprising that no industrial benchmark
appears. A comment on this would be welcome.

Table 4 requires more explanations. Is the CPU time given in seconds?
I understand that the time reported is the cumulated time over all
solved instance. Several solvers seem to be very fast (for example
Asterix, in the SLL => division, solves 292 instances in 2.98 seconds,
which is approximately 10 ms per instance). This is quite
surprising. Does this mean that most instances are small and very
easy? It is also important to recall in this table the total number of
instances. In the two SLL divisions, 3 solvers were able to solve
every instance. Are the problems too easy? In the UDB => division,
sleek solves more instances than cyclist-sl, but does not appear in
bold face. Does it mean that it was incorrect on a few instances? If
some solvers were unsound in some categories, this should be reported.

Can we tell from Table 4 if some techniques are more successful or
promising than others?

How do you plan to assign a measure of difficulty to each instance?
Most instances seem to be quite easy (from Table 4). Don't you need
more challenging/industrial instances?

The sentence "The benchmark problems used in the competition were
split into five divisions that correspond to the three classes of
problems above and the two decision problems considered." is so
surprising (5!=3*2) that I immediately stopped and missed the
explanation in the next sentence. I suggest rewriting it (for example
: "out of the 6 possible divisions (three classes of problems above
and the two decision problems considered), only five were opened
because...").

The limits (2400 s and 100GB) imposed to a solver are unclear to
me. Is it 2400 s CPU time or wall-clock time (BTW, is there any
parallel solver?)? Is it 100GB RAM (huge!) or disk space?

I think it's worth indicating which parts of the format need some
readjustments (what are the expected evolutions ?).

The description of the problem raised by SeLoger (batch mode) is unclear.



